{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client, wrappers\n",
    "from neo4j import GraphDatabase\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT, CONCISENESS_PROMPT, HALLUCINATION_PROMPT\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from langchain.schema.messages import HumanMessage,SystemMessage , AIMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from src.agent import graph\n",
    "from src.agent import graph\n",
    "\n",
    "load_dotenv(find_dotenv()) # read local .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM and Database clients\n",
    "driver = GraphDatabase.driver(uri=os.getenv(\"Neo4j_URL\"), auth=(os.getenv(\"Neo4j_USERNAME\"), os.getenv(\"Neo4j_PASSWORD\")))\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-4o-mini\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define the input and reference output pairs that you'll use to evaluate your app\n",
    "client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"match (n:QA)<-[:HAS_QA]-(d:Document) return n.question as question, n.answer  as answer, d.filename as filename\"\"\"\n",
    "\n",
    "result = driver.execute_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        records = result.records if hasattr(result, 'records') else result\n",
    "       \n",
    "        filenames = []\n",
    "\n",
    "        \n",
    "        datadf = pd.DataFrame()\n",
    "        for record in records:\n",
    "            if hasattr(record, 'data'):\n",
    "                data = {\n",
    "                    \"question\": record.data().get('question'),\n",
    "                    \"answer\": record.data().get('answer'),\n",
    "                    \"filename\": record.data().get('filename')\n",
    "                }\n",
    "                datadf = pd.concat([datadf, pd.DataFrame([data])], ignore_index=True)\n",
    "            \n",
    "            elif isinstance(record, dict):\n",
    "                data = {\n",
    "                    \"question\": record.get('question'),\n",
    "                    \"answer\": record.get('answer'),\n",
    "                    \"filename\": record.get('filename')\n",
    "                }\n",
    "                datadf = pd.concat([datadf, pd.DataFrame([data])], ignore_index=True)\n",
    "            else:\n",
    "                print(f\"Unexpected record type: {type(record)}\")\n",
    "except Exception as e:\n",
    "        filename_list = f\"Error retrieving data: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=\"ConvFinQA dataset\", description=\"A sample ConvFinQA dataset in LangSmith.\"\n",
    ")\n",
    "\n",
    "# Create examples in the dataset. Examples consist of inputs and reference outputs \n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": row[\"question\"]},\n",
    "        \"outputs\": {\"answer\": row[\"answer\"]},\n",
    "        \"filename\": {\"filename\": row[\"filename\"]},\n",
    "    }\n",
    "    for _, row in datadf.iterrows()\n",
    "]\n",
    "\n",
    "# Add the examples to the dataset\n",
    "client.create_examples(dataset_id=dataset.id, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe114888",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = wrappers.wrap_openai(OpenAI())\n",
    "      \n",
    "# Define the application logic you want to evaluate inside a target function. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application\n",
    "# The SDK will automatically send the inputs from the dataset to your target function\n",
    "def target(inputs: dict) -> dict:\n",
    "    \n",
    "    response = graph.invoke(\n",
    "            {\"messages\": [HumanMessage(content=inputs[\"question\"])]}, \n",
    "            config={\"model\": \"gpt-4o-mini\", \"temperature\": 0.7, \"openai_api_key\": os.getenv(\"OPENAI_API_KEY\")}\n",
    "        )\n",
    "\n",
    "    content_str = response['messages'][0][0].content\n",
    "    content_dict = ast.literal_eval(content_str)\n",
    "    answer = content_dict['answer']\n",
    "    return { \"answer\": answer }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an LLM as a judge evaluator to evaluate correctness of the output\n",
    "# Import a prebuilt evaluator prompt from openevals (https://github.com/langchain-ai/openevals) and create an evaluator.\n",
    "    \n",
    "def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    evaluator = create_llm_as_judge(\n",
    "        prompt=CORRECTNESS_PROMPT,\n",
    "        model=\"openai:o3-mini\",\n",
    "        feedback_key=\"correctness\",\n",
    "    )\n",
    "    eval_result = evaluator(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9db737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    evaluator = create_llm_as_judge(\n",
    "        prompt=HALLUCINATION_PROMPT,\n",
    "        model=\"openai:o3-mini\",\n",
    "        feedback_key=\"hallucination\",\n",
    "    )\n",
    "    eval_result = evaluator(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    evaluator = create_llm_as_judge(\n",
    "        prompt=CONCISENESS_PROMPT,\n",
    "        model=\"openai:o3-mini\",\n",
    "        feedback_key=\"conciseness\",\n",
    "    )\n",
    "    eval_result = evaluator(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=\"ConvFinQA dataset\",\n",
    "    evaluators=[\n",
    "        correctness_evaluator,\n",
    "    #    hallucination_evaluator,\n",
    "        conciseness_evaluator, \n",
    "    ],\n",
    "    experiment_prefix=\"ConvFinQA-eval\",\n",
    "    max_concurrency=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance_bot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
